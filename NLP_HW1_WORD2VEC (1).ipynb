{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "NLP_HW1 WORD2VEC ",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEUjNrhz3p-y"
   },
   "source": [
    "# Word Embeddings\n",
    "    More details in the official documentation: https://radimrehurek.com/gensim/auto_examples/index.html#documentation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVVgZCg0RUox",
    "outputId": "c61da97d-80c2-4055-fd5e-43a769856b16"
   },
   "source": [
    "!pip install gensim --upgrade\n",
    "!pip install numpy --upgrade"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in a:\\anaconda\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: Cython==0.29.23 in a:\\anaconda\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: scipy>=0.18.1 in a:\\anaconda\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in a:\\anaconda\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in a:\\anaconda\\lib\\site-packages (from gensim) (1.21.4)\n",
      "Requirement already satisfied: numpy in a:\\anaconda\\lib\\site-packages (1.21.4)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PJ6gpAJCop9D"
   },
   "source": [
    "from gensim import downloader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Itfu9O_nd33A"
   },
   "source": [],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NY4Krmgt7t-y",
    "outputId": "cfc3abc6-9963-4e91-c725-f36dedf7f215"
   },
   "source": [
    "import gensim\n",
    "print(gensim.__version__)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6G8nu1W3wtX"
   },
   "source": [
    "## Loading The Pretrained Weights\n",
    "Supported options are at https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1ieBE5Zo7-iv"
   },
   "source": [
    "import matplotlib as plt\n",
    "WORD_2_VEC_PATH = 'word2vec-google-news-300'\n",
    "GLOVE_PATH = 'glove-twitter-200'\n",
    "GLOVE_PATH2= 'glove-wiki-gigaword-300'"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qL2r1wFarUIM"
   },
   "source": [
    "Stav uses Glove_path\n",
    "\n",
    "Doron uses Glove_path2 - Was pretty bad\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TqRr4ctUpAK4"
   },
   "source": [
    "glove = downloader.load(GLOVE_PATH)\n",
    "#glove2=downloader.load(GLOVE_PATH2)\n",
    "#word2vec=donwnloader.load(WORD_2_VEC_PATH)"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G5hXvcZwSoFY"
   },
   "source": [],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oOjRLg-cOxUd"
   },
   "source": [
    "\n",
    "train_path = 'train.tagged'\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    train_sentences = f.readlines()\n",
    "train_sentences = [sen.strip().lower() for sen in train_sentences]\n",
    "train_sentences = [sen.split() for sen in train_sentences if sen]\n",
    "\n",
    "dev_path = 'dev.tagged'\n",
    "with open(dev_path, 'r', encoding='utf-8') as f:\n",
    "    dev_sentences = f.readlines()\n",
    "dev_sentences = [sen.strip().lower() for sen in dev_sentences]\n",
    "dev_sentences = [sen.split() for sen in dev_sentences if sen]\n",
    "\n"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HPewyExWCKQ"
   },
   "source": [
    "# Preprocessing the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NEmhSIsi0Yw"
   },
   "source": [
    "Let's start with preprocessing the train data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AmOz0hqQR6EJ"
   },
   "source": [
    "words=[]\n",
    "labels=[]\n",
    "for i,word in enumerate(train_sentences):\n",
    "  words.append(word[0])\n",
    "  if word[1]==\"o\":\n",
    "      labels.append(False)\n",
    "  else:\n",
    "      labels.append(True)\n"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHY2UkSeWOhR",
    "outputId": "b4d27849-8038-486c-936e-146cc206ad79"
   },
   "source": [
    "x_train = []\n",
    "y_train=[]\n",
    "miss_words_train=[]\n",
    "for loc,word in enumerate(words):\n",
    "    if word not in glove.key_to_index:\n",
    "        #print(f\"{word} not an existing word in the model\")\n",
    "        miss_words_train.append([labels[loc]])\n",
    "        x_train.append(np.zeros((200,)))\n",
    "        y_train.append(False)\n",
    "        \n",
    "        continue\n",
    "    vec = glove[word]\n",
    "    x_train.append(vec)\n",
    "    y_train.append(labels[loc])\n",
    "x_train = np.asarray(x_train)\n",
    "y_train=np.asarray(y_train)\n",
    "print(x_train.shape,y_train.shape)\n",
    "df=pd.DataFrame(miss_words_train)\n",
    "df.value_counts()"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46469, 200) (46469,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "False    5586\nTrue      176\ndtype: int64"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EyXmaD70xzHw"
   },
   "source": [],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nGp3-9giMbm"
   },
   "source": [
    "now let's do the same for the Dev Set "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KtuVm6ali9jn"
   },
   "source": [
    "words=[]\n",
    "labels=[]\n",
    "for i,word in enumerate(dev_sentences):\n",
    "  words.append(word[0])\n",
    "  if word[1]==\"o\":\n",
    "      labels.append(False)\n",
    "  else:\n",
    "      labels.append(True)\n"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wFA9r07BjAH6",
    "outputId": "badec200-e362-4f22-d0cb-77adf720f78d"
   },
   "source": [
    "x_val = []\n",
    "y_val=[]\n",
    "miss_words_dev=[]\n",
    "\n",
    "for loc,word in enumerate(words):\n",
    "    if word not in glove.key_to_index:\n",
    "        ##print(f\"{word} not an existing word in the model\")\n",
    "        x_val.append(np.zeros((200,)))\n",
    "        y_val.append(False)\n",
    "        miss_words_dev.append(labels[loc])\n",
    "        continue\n",
    "    vec = glove[word]\n",
    "    x_val.append(vec)\n",
    "    y_val.append(labels[loc])\n",
    "x_val = np.asarray(x_val)\n",
    "y_val=np.asarray(y_val)\n",
    "print(x_val.shape,y_val.shape)\n",
    "df=pd.DataFrame(miss_words_dev)\n",
    "df.value_counts()"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16261, 200) (16261,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "False    3011\nTrue      115\ndtype: int64"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdXnZNRsjqb6"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTqXM4RbjsyW"
   },
   "source": [
    "**AdaBoost Model**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0Wmjgx4ob6wD"
   },
   "source": [
    "clf = AdaBoostClassifier(n_estimators=1000,random_state=20) # 8 mins for 1000 est\n",
    "clf.fit(x_train, y_train)\n",
    "train_y_pred=clf.predict(x_train)\n",
    "val_y_pred=clf.predict(x_val)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IO6CP2Vpgn3y",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "675476b3-bbbf-4f39-b7a0-37d20b6b430f"
   },
   "source": [
    "\n",
    "print(f'F1 score for the train {f1_score(y_train, train_y_pred)}')\n",
    "print(f'F1 score for the val {f1_score(y_val, val_y_pred)}')\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score for the train 0.7646026573075959\n",
      "F1 score for the val 0.528658875904285\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xYf0XTd4nbMg"
   },
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9Fy0o7ThmAIs"
   },
   "source": [
    "\"\"\"from sklearn.model_selection import  GridSearchCV\n",
    "search_grid={'n_estimators':[1200,1500,2000],\n",
    "             'learning_rate':[0.5,0.8,1]}\n",
    "search=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=search_grid,scoring='f1',n_jobs=1,cv=2,verbose=3)\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "search.fit(x_train, y_train)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable\n",
    "print(search.best_estimator_)\n",
    "print(search.best_params_)\n",
    "\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-__dPXetxA8c"
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJjaXNV5npPM",
    "outputId": "bc521bb2-b1e6-4728-ad61-e63b04f24704"
   },
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "svm_model = svm.SVC(random_state=20,C=2)\n",
    "svm_model.fit(x_train, y_train)\n",
    "val_y_pred_svm=svm_model.predict(x_val)\n",
    "train_y_pred_svm=svm_model.predict(x_train)\n",
    "\n",
    "print(f'F1 score for the train {f1_score(y_train, train_y_pred_svm)}')\n",
    "print(f'Accuracy score for the train {accuracy_score(y_train, train_y_pred_svm)}')\n",
    "\n",
    "print(f'F1 score for the val {f1_score(y_val, val_y_pred_svm)}')\n",
    "print(f'Accuracy score for the val {accuracy_score(y_val, val_y_pred_svm)}')\n",
    "\n"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score for the train 0.8147227057945784\n",
      "Accuracy score for the train 0.9839678064946523\n",
      "F1 score for the val 0.6288009179575446\n",
      "Accuracy score for the val 0.9602115491052211\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1LJV3DtwlvQ"
   },
   "source": [
    "C=2\n",
    "\n",
    "F1 score for the train 0.789580171977744\n",
    "\n",
    "Accuracy score for the train 0.9820955906087929\n",
    "\n",
    "F1 score for the val 0.6362038664323374\n",
    "\n",
    "Accuracy score for the val 0.9618104667609618\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eqzzf7IA1Q4e"
   },
   "source": [
    "**Grid Search for SVM**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgPVyBHx1QCI",
    "outputId": "3042ae14-15f2-45d2-b5d9-d7c09c5d362c"
   },
   "source": [
    "from sklearn.model_selection import  GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {'C':[2,3,4,5,6,7,10,20],'gamma':['scale','auto'],  'kernel':['rbf']}\n",
    "grid = GridSearchCV(svm.SVC(),param_grid,refit = True, verbose=3,scoring='f1', cv=4)\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "\n",
    "grid.fit(x_train,y_train)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_params_)\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "[CV 1/4] END .C=0.1, gamma=scale, kernel=linear;, score=0.557 total time=  35.1s\n",
      "[CV 2/4] END .C=0.1, gamma=scale, kernel=linear;, score=0.567 total time=  35.0s\n",
      "[CV 3/4] END .C=0.1, gamma=scale, kernel=linear;, score=0.542 total time=  34.8s\n",
      "[CV 4/4] END .C=0.1, gamma=scale, kernel=linear;, score=0.562 total time=  35.8s\n",
      "[CV 1/4] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.385 total time=  36.0s\n",
      "[CV 2/4] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.414 total time=  36.3s\n",
      "[CV 3/4] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.440 total time=  36.3s\n",
      "[CV 4/4] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.435 total time=  36.9s\n",
      "[CV 1/4] END ..C=0.1, gamma=auto, kernel=linear;, score=0.557 total time=  36.0s\n",
      "[CV 2/4] END ..C=0.1, gamma=auto, kernel=linear;, score=0.567 total time=  36.0s\n",
      "[CV 3/4] END ..C=0.1, gamma=auto, kernel=linear;, score=0.542 total time=  35.9s\n",
      "[CV 4/4] END ..C=0.1, gamma=auto, kernel=linear;, score=0.562 total time=  36.9s\n",
      "[CV 1/4] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.024 total time=  35.8s\n",
      "[CV 2/4] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.028 total time=  35.9s\n",
      "[CV 3/4] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.034 total time=  36.8s\n",
      "[CV 4/4] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.017 total time=  36.3s\n",
      "[CV 1/4] END ...C=1, gamma=scale, kernel=linear;, score=0.555 total time= 1.6min\n",
      "[CV 2/4] END ...C=1, gamma=scale, kernel=linear;, score=0.566 total time= 1.7min\n",
      "[CV 3/4] END ...C=1, gamma=scale, kernel=linear;, score=0.541 total time= 1.4min\n",
      "[CV 4/4] END ...C=1, gamma=scale, kernel=linear;, score=0.551 total time= 1.7min\n",
      "[CV 1/4] END ......C=1, gamma=scale, kernel=rbf;, score=0.618 total time=  44.9s\n",
      "[CV 2/4] END ......C=1, gamma=scale, kernel=rbf;, score=0.619 total time=  49.2s\n",
      "[CV 3/4] END ......C=1, gamma=scale, kernel=rbf;, score=0.636 total time=  48.1s\n",
      "[CV 4/4] END ......C=1, gamma=scale, kernel=rbf;, score=0.644 total time=  48.0s\n",
      "[CV 1/4] END ....C=1, gamma=auto, kernel=linear;, score=0.555 total time= 1.6min\n",
      "[CV 2/4] END ....C=1, gamma=auto, kernel=linear;, score=0.566 total time= 1.7min\n",
      "[CV 3/4] END ....C=1, gamma=auto, kernel=linear;, score=0.541 total time= 1.4min\n",
      "[CV 4/4] END ....C=1, gamma=auto, kernel=linear;, score=0.551 total time= 1.7min\n",
      "[CV 1/4] END .......C=1, gamma=auto, kernel=rbf;, score=0.550 total time=  34.0s\n",
      "[CV 2/4] END .......C=1, gamma=auto, kernel=rbf;, score=0.553 total time=  34.4s\n",
      "[CV 3/4] END .......C=1, gamma=auto, kernel=rbf;, score=0.551 total time=  33.8s\n",
      "[CV 4/4] END .......C=1, gamma=auto, kernel=rbf;, score=0.570 total time=  34.2s\n",
      "[CV 1/4] END ..C=10, gamma=scale, kernel=linear;, score=0.555 total time=12.1min\n",
      "[CV 2/4] END ..C=10, gamma=scale, kernel=linear;, score=0.566 total time=12.2min\n",
      "[CV 3/4] END ..C=10, gamma=scale, kernel=linear;, score=0.544 total time=11.1min\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMevi71wp6bG"
   },
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "RBp8uic1mp9g",
    "outputId": "b254a228-4c23-4fbb-fd56-2d30d436da36"
   },
   "source": [
    " from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    " neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    " neigh.fit(x_train, y_train)\n",
    " train_y_pred_neigh=neigh.predict(x_train)\n",
    " val_y_pred_neigh=neigh.predict(x_val)\n",
    "\n",
    " print(f'F1 score for the train {f1_score(y_train,  train_y_pred_neigh)}')\n",
    " print(f'F1 score for the val {f1_score(y_val, val_y_pred_neigh)}')\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score for the train 0.634770514603616\n",
      "F1 score for the val 0.5026041666666666\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nin K=3\\nF1 score for the train 0.6966982339390837\\nF1 score for the val 0.46782316731952994\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nv9nm9NdxY9r"
   },
   "source": [
    "F1 score for the train 0.634770514603616\n",
    "\n",
    "F1 score for the val 0.5026041666666666"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bUHSFd67r8hh"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0vDPssRonjl",
    "outputId": "2a69acc2-97cf-4853-dc1b-ee26a40849bf"
   },
   "source": [
    "\"\"\"from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100,random_state=20)\n",
    "random_forest_model.fit(x_train, y_train)\n",
    "val_y_pred_tree=random_forest_model.predict(x_val)\n",
    "print(f'F1 score for the val {f1_score(y_val, val_y_pred_tree)}')\n",
    "\n",
    "\"\"\""
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score for the val 0.4069015097052479\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7M1K6qoL2IX"
   },
   "source": [
    "# Model 2 Liner ANN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c4JXt6X7NYsW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6c136aad-2f1d-4a5b-e8ca-487dcfc1af5c"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# We create a FC regression network, with 2 layers.\n",
    "class RegressioNet(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(RegressioNet, self).__init__()\n",
    "       self.hidden_dim = 20\n",
    "       self.layer_1 = torch.nn.Linear(200, self.hidden_dim)\n",
    "       self.layer_2 = torch.nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "   def forward(self, x):\n",
    "       x = self.layer_1(x)        # x.size() -> [batch_size, self.hidden_dim]\n",
    "       x = self.layer_2(x)        # x.size() -> [batch_size, 1]\n",
    "\n",
    "       return x  \n",
    "\n",
    "net = RegressioNet()\n",
    "print(net)"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressioNet(\n",
      "  (layer_1): Linear(in_features=200, out_features=20, bias=True)\n",
      "  (layer_2): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5L6ICCxLP6fT"
   },
   "source": [
    "# Define Optimizer and Loss Function\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a87Mh9XhTlsc"
   },
   "source": [
    "batch_size = 20\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).float())\n",
    "valid_data = TensorDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val).float())\n",
    "\n",
    "#test_data = TensorDataset(torch.from_numpy(test_x).float(), torch.from_numpy(test_y).float())\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n",
    "#test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ig2r3Be5VVzj",
    "outputId": "34ec7d2d-2049-40ef-9f20-7499e33117f6"
   },
   "source": [
    "# First checking if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aNJ7WgXwVjEV"
   },
   "source": [
    "# Define training params\n",
    "epochs = 10\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 1000 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "net = net.float()\n",
    "net.to(device)\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "RegressioNet(\n  (layer_1): Linear(in_features=200, out_features=20, bias=True)\n  (layer_2): Linear(in_features=20, out_features=1, bias=True)\n)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_25104/1236745482.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m         \u001B[1;31m# if training on gpu\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m         \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m         \u001B[1;31m# zero accumulated gradients\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        # if training on gpu\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        # x.size() -> [batch_size]\n",
    "        batch_size = inputs.size(0)\n",
    "        # IMPORTANT - change the dimensions of x before it enters the NN, batch size must always be first\n",
    "        x = inputs.unsqueeze(0)         # x.size() -> [1, batch_size]\n",
    "        x = x.view(batch_size, -1)      # x.size() -> [batch_size, 1]\n",
    "        predictions = net(x)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = loss_func(predictions.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            print_flag = True\n",
    "            for inputs, labels in valid_loader:\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                if print_flag:\n",
    "                  inputs, labels = zip(*sorted(zip(inputs.numpy(), labels.numpy())))\n",
    "                  inputs = torch.from_numpy(np.asarray(inputs))\n",
    "                  labels = torch.from_numpy(np.asarray(labels))\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "                # get the output from the model\n",
    "                # x.size() -> [batch_size]\n",
    "                batch_size = inputs.size(0)\n",
    "                # IMPORTANT - change the dimensions of x before it enters the NN, batch size must always be first\n",
    "                x = inputs.unsqueeze(0)    # x.size() -> [1, batch_size]\n",
    "                x = x.view(batch_size, -1) # x.size() -> [batch_size, 1]\n",
    "                val_predictions = net(x)\n",
    "                val_loss = loss_func(val_predictions.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "                if print_flag:\n",
    "                  print_flag = False\n",
    "                  # plot and show learning process\n",
    "                  fig = plt.figure()\n",
    "                  ax = fig.add_subplot(111)\n",
    "                  ax.cla()\n",
    "                  ax.scatter(inputs.cpu().data.numpy(), labels.cpu().data.numpy())\n",
    "                  ax.plot(inputs.cpu().data.numpy(), val_predictions.cpu().data.numpy(), 'r-', lw=2)\n",
    "                  ax.text(0.5, 0, 'Loss=%.4f' % np.mean(val_losses), fontdict={'size': 10, 'color':  'red'})\n",
    "                  plt.pause(0.1)\n",
    "                  ax.clear()\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}